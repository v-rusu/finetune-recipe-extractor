# W&B Sweep Configuration for Recipe Extractor Finetuning
#
# Usage:
#   1. Create the sweep:
#      wandb sweep sweep.yaml
#
#   2. Run sweep agent(s):
#      wandb agent <sweep_id>
#
#      Or with the script:
#      python 06_finetune_wandb.py --sweep-id <sweep_id> --sweep-count 10
#
#   3. (Optional) Run multiple agents in parallel on different machines

program: 06_finetune_wandb.py
method: bayes  # Options: grid, random, bayes
metric:
  name: final_train_loss
  goal: minimize

# Fixed parameters (not swept)
parameters:
  dataset:
    value: "./finetuning_dataset.jsonl"
  model_name:
    value: "unsloth/gemma-3-270m-it"
  max_seq_length:
    value: 8192
  max_steps:
    value: -1  # Full epochs
  num_epochs:
    value: 1
  output_dir:
    value: "outputs"
  optimizer:
    value: "adamw_8bit"

  # Hyperparameters to sweep
  learning_rate:
    distribution: log_uniform_values
    min: 1e-6
    max: 1e-4

  lora_rank:
    values: [32, 64, 128, 256]

  lora_alpha:
    values: [32, 64, 128, 256]

  lora_dropout:
    values: [0, 0.05, 0.1]

  batch_size:
    values: [2, 4, 8]

  gradient_accumulation_steps:
    values: [2, 4, 8]

  weight_decay:
    distribution: log_uniform_values
    min: 1e-4
    max: 1e-2

  warmup_steps:
    values: [0, 5, 10, 20]

  lr_scheduler_type:
    values: ["linear", "cosine", "constant_with_warmup"]

  use_rslora:
    values: [false, true]

# Early termination for poor runs
early_terminate:
  type: hyperband
  min_iter: 10
  eta: 3
  s: 2
